<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>初探强化学习 | Doraemon's Blog</title><meta name="author" content="Doraemon,601162956@qq.com"><meta name="copyright" content="Doraemon"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文介绍了强化学习的思想和适应性从弱到强的几种强化学习算法，包括Sarsa、QLearning、DQN、DDPG。并利用这些算法在OpenAI的gym实验室中解决了实际的问题，可以帮助小白入门强化学习。">
<meta property="og:type" content="article">
<meta property="og:title" content="初探强化学习">
<meta property="og:url" content="http://fezhu.top/posts/3ff4588e.html">
<meta property="og:site_name" content="Doraemon&#39;s Blog">
<meta property="og:description" content="本文介绍了强化学习的思想和适应性从弱到强的几种强化学习算法，包括Sarsa、QLearning、DQN、DDPG。并利用这些算法在OpenAI的gym实验室中解决了实际的问题，可以帮助小白入门强化学习。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/uncleacc/Img2/130.webp">
<meta property="article:published_time" content="2025-01-11T10:07:37.000Z">
<meta property="article:modified_time" content="2025-09-14T08:45:56.100Z">
<meta property="article:author" content="Doraemon">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/uncleacc/Img2/130.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://fezhu.top/posts/3ff4588e.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//www.clarity.ms"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="KN6pnEmNs8iAPCiA4HK81_JOnb5b90LWJLxqyZdH0ko"/><meta name="baidu-site-verification" content="codeva-fP6p2S4Tpw"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?e5f1c961209bbe62fbceb50c9ef96345";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BDW0BKMDBE"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BDW0BKMDBE');
</script><script>(function(c,l,a,r,i,t,y){
    c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
    t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
    y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
})(window, document, "clarity", "script", "jec82yg9m8");</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '初探强化学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-14 16:45:56'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/uncleacc/sucai_2/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">111</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/talk/"><i class="fa-fw fa-regular fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/uncleacc/Img2/130.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="Doraemon's Blog"><img class="site-icon" src="https://cdn.jsdelivr.net/gh/uncleacc/website_materials_img/blogIcon.png"/><span class="site-name">Doraemon's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/talk/"><i class="fa-fw fa-regular fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">初探强化学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-11T10:07:37.000Z" title="发表于 2025-01-11 18:07:37">2025-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-14T08:45:56.100Z" title="更新于 2025-09-14 16:45:56">2025-09-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/posts/3ff4588e.html" data-flag-title="初探强化学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/3ff4588e.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/3ff4588e.html" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文介绍了强化学习的思想和适应性从弱到强的几种强化学习算法，包括Sarsa、QLearning、DQN、DDPG。并利用这些算法在OpenAI的gym实验室中解决了实际的问题，可以帮助小白入门强化学习。</p>
<h1 id="强化学习思想"><a href="#强化学习思想" class="headerlink" title="强化学习思想"></a>强化学习思想</h1><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p><img src="https://pic2.zhimg.com/v2-ccf370a53e34d7f07c9f8a0baa081f0d_1440w.jpg" alt="img"></p>
<p><strong>智能体（Agent）</strong>：智能体是与环境交互的主体，通过观察环境状态、采取动作并根据奖励信号学习策略。</p>
<p><strong>环境（Environment）</strong>：智能体所处的外部世界，它根据智能体的动作提供新的状态和奖励。</p>
<p><strong>状态(State)/观察值(Observation)</strong>：环境的一个描述，包含智能体当前所处的所有信息。可以是观测到的部分状态（部分可观测环境）或完整状态。</p>
<p><strong>动作（Action）</strong>：智能体在某一状态下可以执行的行为，影响环境的转变。动作空间可以是<strong>离散的</strong>或<strong>连续的</strong>。例如，走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间;如果机器人向 360◦ 中的任意角度都可以移动，则为连续动作空间。</p>
<p><strong>奖励（Reward）</strong>：智能体每执行一个动作后，环境反馈的数值信号，用来表示该动作的好坏。奖励可以是<strong>即时奖励</strong>或<strong>累积奖励</strong>。</p>
<p><strong>策略（Policy）</strong>：定义智能体在某一状态下选择动作的行为规则。策略可以是：</p>
<ul>
<li><strong>随机策略（Stochastic Policy）</strong>：输出动作的概率分布。</li>
<li><strong>确定性策略（Deterministic Policy）</strong>：直接输出具体动作。</li>
</ul>
<p><strong>价值函数（Value Function）</strong>：描述一个状态或状态-动作对的好坏，衡量长期回报的期望值。</p>
<ul>
<li><p><strong>状态价值函数 V(s)</strong>：用来度量给定策略π的情况下，当前状态st的好坏程度。</p>
</li>
<li><p><strong>动作价值函数 Q(s,a)</strong>：用来度量给定状态$s_t$和策略$π$的情况下，采用动作$a_t$的好坏程度。</p>
</li>
</ul>
<p><strong>折扣因子（Discount Factor）</strong>：衡量未来奖励的重要性，取值范围 $0 \leq \gamma \leq 1$。较低的 $\gamma$ 更关注短期奖励，较高的 $\gamma$ 更关注长期奖励。</p>
<h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>强化学习（Reinforcement learning，RL）讨论的问题是一个<strong>智能体(agent)</strong> 怎么在一个复杂不确定的 <strong>环境(environment)</strong> 里面去极大化它能获得的奖励。通过感知所处环境的 <strong>状态(state)</strong> 对 <strong>动作(action)</strong> 的 <strong>反应(reward)</strong>， 来指导更好的动作，从而获得最大的 <strong>收益(return)</strong>，这被称为在交互中学习，这样的学习方法就被称作强化学习</p>
<p><img src="https://picx.zhimg.com/v2-8bbe0cc23c0c0b104aa44089111b6e8b_1440w.jpg" alt="img"></p>
<p>在强化学习过程中，智能体跟环境一直在交互。智能体在环境里面获取到状态，智能体会利用这个状态输出一个动作，一个决策。然后这个决策会放到环境之中去，环境会根据智能体采取的决策，输出下一个状态以及当前的这个决策得到的奖励。智能体的目的就是为了尽可能多地从环境中获取奖励。</p>
<p>强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。</p>
<ul>
<li><strong>监督学习</strong> 是从外部监督者提供的带标注训练集中进行学习。 <strong>(任务驱动型)</strong></li>
<li><strong>非监督学习</strong> 是一个典型的寻找未标注数据中隐含结构的过程。 <strong>(数据驱动型)</strong></li>
<li><strong>强化学习</strong> 更偏重于智能体与环境的交互， 这带来了一个独有的挑战 ——“<strong>试错（exploration）</strong>”与“<strong>开发（exploitation）</strong>”之间的折中权衡，智能体必须开发已有的经验来获取收益，同时也要进行试探，使得未来可以获得更好的动作选择空间。 <strong>(从错误中学习)</strong></li>
</ul>
<p>强化学习主要有以下几个特点：</p>
<ul>
<li><strong>试错学习</strong>：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。</li>
<li><strong>延迟回报</strong>：强化学习的指导信息很少，而且往往是在事后（最后一个状态(State)）才给出的。比如 围棋中只有到了最后才能知道胜负。</li>
</ul>
<p><strong>按照学习目标划分：基于策略（Policy-Based）和基于价值（Value-Based）。</strong></p>
<p><img src="https://pic1.zhimg.com/v2-00c43d39fee17d31da576e6e0a7df99e_1440w.jpg" alt="img"></p>
<ul>
<li><strong>Policy-Based</strong>的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。</li>
<li><strong>Value-Based</strong>的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa。</li>
<li>更为厉害的方法是二者的结合：Actor-Critic，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程，常见的有A2C，A3C，DDPG等。</li>
</ul>
<h1 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h1><h2 id="Sarsa-悬崖问题"><a href="#Sarsa-悬崖问题" class="headerlink" title="Sarsa (悬崖问题)"></a>Sarsa (悬崖问题)</h2><p><img src="初探强化学习.assets/image-20250111221018607.png" alt="image-20250111221018607"></p>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SarsaAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, obs_n, act_n, learning_rate=<span class="number">0.01</span>, gamma=<span class="number">0.9</span>, e_greed=<span class="number">0.1</span></span>):</span><br><span class="line">        self.act_n = act_n</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.epsilon = e_greed</span><br><span class="line">        self.Q = np.zeros((obs_n, act_n))</span><br><span class="line">    <span class="comment"># e_greed:根据s_t,选择a_t</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self,obs</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.uniform(<span class="number">0</span>,<span class="number">1</span>) &lt; (<span class="number">1.0</span> - self.epsilon):</span><br><span class="line">            action = self.predict(obs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.choice(self.act_n) <span class="comment"># 0,1,2,3</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment"># a_t = argmax Q(s)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, obs</span>):</span><br><span class="line">        Q_list = self.Q[obs, :] <span class="comment">#当前s下所有a对应的Q值</span></span><br><span class="line">        maxQ = np.<span class="built_in">max</span>(Q_list)</span><br><span class="line">        action_list = np.where(Q_list == maxQ)[<span class="number">0</span>] <span class="comment"># action_list=所有=Qmax的索引</span></span><br><span class="line">        action = np.random.choice(action_list)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self, obs, action, reward, next_obs, next_action, done</span>): <span class="comment"># (S,A,R,S,A)</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        done: episode是否结束</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        predict_Q = self.Q[obs,action]</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            target_Q = reward</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_Q = reward + self.gamma * self.Q[next_obs,next_action]</span><br><span class="line">        <span class="comment"># 更新Q表格</span></span><br><span class="line">        self.Q[obs,action] += self.lr * (target_Q - predict_Q)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self</span>):</span><br><span class="line">        npy_file = <span class="string">&#x27;./q-table.npy&#x27;</span></span><br><span class="line">        np.save(npy_file, self.Q)</span><br><span class="line">        <span class="built_in">print</span>(npy_file + <span class="string">&#x27; saved.&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, npy_file=<span class="string">&#x27;./q_table.npy&#x27;</span></span>):</span><br><span class="line">        self.Q = np.load(npy_file)</span><br><span class="line">        <span class="built_in">print</span>(npy_file + <span class="string">&#x27; loaded.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_episode</span>(<span class="params">env, agent, render=<span class="literal">False</span></span>):</span><br><span class="line">    total_steps = <span class="number">0</span> <span class="comment"># 记录当前episode走了多少step</span></span><br><span class="line">    total_reward = <span class="number">0</span> </span><br><span class="line">    obs = env.reset()</span><br><span class="line">    action = agent.sample(obs)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        next_obs, reward, done, _ = env.step(action)</span><br><span class="line">        next_action = agent.sample(next_obs)</span><br><span class="line">        agent.learn(obs, action, reward, next_obs, next_action, done)</span><br><span class="line">        action = next_action</span><br><span class="line">        obs = next_obs</span><br><span class="line">        total_reward += reward</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> render:</span><br><span class="line">            env.render()</span><br><span class="line">            time.sleep(<span class="number">0.</span>)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_reward, total_steps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_episode</span>(<span class="params">env, agent</span>): </span><br><span class="line">    total_steps = <span class="number">0</span> <span class="comment"># 记录当前episode走了多少step</span></span><br><span class="line">    total_reward = <span class="number">0</span> </span><br><span class="line">    obs = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        action = agent.predict(obs)</span><br><span class="line">        next_obs, reward, done, _ = env.step(action)</span><br><span class="line">        total_reward += reward</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">        obs = next_obs</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        env.render()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_reward, total_steps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    env = gym.make(<span class="string">&quot;CliffWalking-v0&quot;</span>)</span><br><span class="line">    agent = SarsaAgent(obs_n=env.observation_space.n, </span><br><span class="line">                       act_n=env.action_space.n,</span><br><span class="line">                       learning_rate=<span class="number">0.025</span>, gamma=<span class="number">0.9</span>, e_greed=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        total_reward, total_steps = run_episode(env, agent, <span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Episode %s: total_steps = %s , total_reward = %.1f&#x27;</span> % (episode, total_steps, total_reward))</span><br><span class="line">    test_episode(env, agent)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>在Q-learning中，我们维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在当前状态S下可以采用动作A可以获得的未来收益的折现和。我们不断的迭代我们的Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。</p>
<p><img src="https://pic1.zhimg.com/v2-6fa4c30d7dfdf02a608c1758fc69edea_1440w.jpg" alt="img"></p>
<p>假设机器人必须越过迷宫并到达终点。有地雷，机器人一次只能移动一个地砖。如果机器人踏上矿井，机器人就死了。机器人必须在尽可能短的时间内到达终点。 得分/奖励系统如下：</p>
<ul>
<li>机器人在每一步都失去1点。这样做是为了使机器人采用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=最短路径&amp;zhida_source=entity">最短路径</a>并尽可能快地到达目标。</li>
<li>如果机器人踩到地雷，则点损失为100并且游戏结束。</li>
<li>如果机器人获得动力⚡️，它会获得1点。</li>
<li>如果机器人达到最终目标，则机器人获得100分。 现在，显而易见的问题是：我们如何训练机器人以最短的路径到达最终目标而不踩矿井？</li>
</ul>
<h3 id="Q值表"><a href="#Q值表" class="headerlink" title="Q值表"></a><strong>Q值表</strong></h3><p>Q值表(Q-Table)是一个简单查找表的名称，我们计算每个状态的最大预期未来奖励。基本上，这张表将指导我们在每个状态采取最佳行动。</p>
<p><img src="https://pic3.zhimg.com/v2-0ae8f9bba40cb934c20978a5d6d95a68_1440w.jpg" alt="img"></p>
<h3 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a><strong>Q函数</strong></h3><p>Q函数(Q-Function)即为上文提到的动作价值函数，他有两个输入：「状态」和「动作」。它将返回在该状态下执行该动作的未来奖励期望。</p>
<p><img src="https://pic3.zhimg.com/v2-45c7656e0245e84a0743cfa73ef17420_1440w.jpg" alt="img"></p>
<p>我们可以把Q函数视为一个在Q-Table上滚动的读取器，用于寻找与当前状态关联的行以及与动作关联的列。它会从相匹配的单元格中返回 Q 值。这就是未来奖励的期望。</p>
<p><img src="https://pic3.zhimg.com/v2-7b3c0c373dccad3cf3d5d4728b356c0c_1440w.jpg" alt="img"></p>
<p>在我们探索环境（environment）之前，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=Q-table&amp;zhida_source=entity">Q-table</a> 会给出相同的任意的设定值（大多数情况下是 0）。随着对环境的持续探索，这个 Q-table 会通过迭代地使用 Bellman 方程（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=动态规划方程&amp;zhida_source=entity">动态规划方程</a>）更新 Q(s，a) 来给出越来越好的近似。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h3><p><img src="https://pic4.zhimg.com/v2-bc3517d0e73e145d525d9542e58b35e3_1440w.jpg" alt="img"></p>
<p><img src="https://pic3.zhimg.com/v2-6c9bc7197c4fe0c110ed3843c8a6fe4e_1440w.jpg" alt="img"></p>
<p><strong>第1步：初始化Q值表</strong> 我们将首先构建一个Q值表。有n列，其中n=操作数。有m行，其中m=状态数。我们将值初始化为0</p>
<p><img src="https://picx.zhimg.com/v2-89f932969cdbb19f8b105e430670b3bd_1440w.jpg" alt="img"></p>
<p><strong>步骤2和3：选择并执行操作</strong> 这些步骤的组合在不确定的时间内完成。这意味着此步骤一直运行，直到我们停止训练，或者训练循环停止。</p>
<p><img src="https://picx.zhimg.com/v2-80ca3bbb7471b464800df1bccfb0dff1_1440w.jpg" alt="img"></p>
<p>如果每个Q值都等于零，我们就需要权衡探索/利用（exploration/exploitation）的程度了，思路就是，在一开始，我们将使用 <a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=epsilon&amp;zhida_source=entity">epsilon</a> 贪婪策略：</p>
<ul>
<li>我们指定一个探索速率「epsilon」，一开始将它设定为 1。这个就是我们将随机采用的步长。在一开始，这个速率应该处于最大值，因为我们不知道 Q-table 中任何的值。这意味着，我们需要通过随机选择动作进行大量的探索。</li>
<li>生成一个随机数。如果这个数大于 epsilon，那么我们将会进行「利用」（这意味着我们在每一步利用已经知道的信息选择动作）。否则，我们将继续进行探索。</li>
<li>在刚开始训练 Q 函数时，我们必须有一个大的 epsilon。随着智能体对估算出的 Q 值更有把握，我们将逐渐减小 epsilon。</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-d94d164f15947a25d50978c942940bf8_1440w.jpg" alt="img"></p>
<p><strong>步骤4和5：评估</strong> 现在我们采取了行动并观察了结果和奖励。我们需要更新功能Q（s，a）：</p>
<p><img src="https://pic1.zhimg.com/v2-62b52d433fc2079274e7affea881ae02_1440w.jpg" alt="img"></p>
<p>最后生成的Q表：</p>
<p><img src="https://pic2.zhimg.com/v2-66d5eccdf6520f8215c684b4016c09df_1440w.jpg" alt="img"></p>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QLearningAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, obs_n, act_n, learning_rate=<span class="number">1e-2</span>, gamma=<span class="number">0.9</span>, e_greed=<span class="number">0.1</span></span>):</span><br><span class="line">        self.act_n = act_n  <span class="comment"># 动作维度，有几个动作可选</span></span><br><span class="line">        self.lr = learning_rate  <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># reward的衰减率</span></span><br><span class="line">        self.epsilon = e_greed  <span class="comment"># 按一定概率随机选动作</span></span><br><span class="line">        self.Q = np.zeros((obs_n, act_n))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, obs</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; (<span class="number">1.0</span> - self.epsilon):  <span class="comment"># 根据table的Q值选动作</span></span><br><span class="line">            action = self.predict(obs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.choice(self.act_n)  <span class="comment"># 有一定概率随机探索选取一个动作</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment"># 根据输入观察值，预测输出的动作值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, obs</span>):</span><br><span class="line">        Q_list = self.Q[obs, :]</span><br><span class="line">        maxQ = np.<span class="built_in">max</span>(Q_list)</span><br><span class="line">        action_list = np.where(Q_list == maxQ)[<span class="number">0</span>]  <span class="comment"># maxQ可能对应多个action</span></span><br><span class="line">        action = np.random.choice(action_list)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self, obs, action, reward, next_obs, done</span>): <span class="comment">#(S,A,R,S)</span></span><br><span class="line">        predict_Q = self.Q[obs, action]</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            target_Q = reward</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_Q = reward + self.gamma * np.<span class="built_in">max</span>(self.Q[next_obs,:])</span><br><span class="line">        self.Q[obs, action] += self.lr * (target_Q - predict_Q)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self</span>):</span><br><span class="line">        npy_file = <span class="string">&#x27;./q-table.npy&#x27;</span></span><br><span class="line">        np.save(npy_file, self.Q)</span><br><span class="line">        <span class="built_in">print</span>(npy_file + <span class="string">&#x27; saved.&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, npy_file=<span class="string">&#x27;./q_table.npy&#x27;</span></span>):</span><br><span class="line">        self.Q = np.load(npy_file)</span><br><span class="line">        <span class="built_in">print</span>(npy_file + <span class="string">&#x27; loaded.&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_episode</span>(<span class="params">env, agent, render=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># 其实真正执行的策略和Sarsa是一样的，只不过学习的策略是保守的最优策略</span></span><br><span class="line">    total_steps = <span class="number">0</span></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line">    obs = env.reset()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        action = agent.sample(obs)</span><br><span class="line">        next_obs, reward, done, _ = env.step(action)</span><br><span class="line">        agent.learn(obs, action, reward, next_obs, done)</span><br><span class="line">        obs = next_obs</span><br><span class="line"></span><br><span class="line">        total_reward += reward</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> render:</span><br><span class="line">            env.render()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_reward, total_steps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_episode</span>(<span class="params">env, agent</span>):</span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line">    obs = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        action = agent.predict(obs)  <span class="comment"># greedy</span></span><br><span class="line">        next_obs, reward, done, _ = env.step(action)</span><br><span class="line">        total_reward += reward</span><br><span class="line">        obs = next_obs</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        env.render()</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_reward</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    env = gym.make(<span class="string">&quot;CliffWalking-v0&quot;</span>)  <span class="comment"># 0 up, 1 right, 2 down, 3 left</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个agent实例，输入超参数</span></span><br><span class="line">    agent = QLearningAgent(</span><br><span class="line">        obs_n=env.observation_space.n,</span><br><span class="line">        act_n=env.action_space.n,</span><br><span class="line">        learning_rate=<span class="number">0.1</span>,</span><br><span class="line">        gamma=<span class="number">0.9</span>,</span><br><span class="line">        e_greed=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练500个episode，打印每个episode的分数</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">        ep_reward, ep_steps = run_episode(env, agent, <span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Episode %s: steps = %s , reward = %.1f&#x27;</span> % (episode, ep_steps, ep_reward))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全部训练结束，查看算法效果</span></span><br><span class="line">    test_reward = test_episode(env, agent)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;test reward = %.1f&#x27;</span> % (test_reward))</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a><strong>Deep Q Network</strong></h2><p>在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=3&amp;q=Q-Table&amp;zhida_source=entity">Q-Table</a>储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不现实，我们无法构建可以存储超大<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=状态空间&amp;zhida_source=entity">状态空间</a>的Q_table。不过，在机器学习中， 有一种方法对这种事情很在行，那就是神经网络，可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样就没必要在表格中记录 Q 值，而是直接使用神经网络预测Q值</p>
<p><img src="https://pica.zhimg.com/v2-cec53729a7c798c93012c478d8086596_1440w.jpg" alt="img"></p>
<h3 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a><strong>经验回放</strong></h3><p>DQN利用Qlearning特点，目标策略与动作策略分离，学习时利用经验池储存的经验取batch更新Q。同时提高了样本的利用率，也打乱了样本状态相关性使其符合神经网络的使用特点。</p>
<h3 id="固定Q目标"><a href="#固定Q目标" class="headerlink" title="固定Q目标"></a><strong>固定Q目标</strong></h3><p>神经网络一般学习的是固定的目标，而Qlearning中Q同样为学习的变化量，变动太大不利于学习。所以DQN使Q在一段时间内保持不变，使神经网络更易于学习。</p>
<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h3><p><img src="https://pica.zhimg.com/v2-1e4d1da2aaf789925227f21d4e6843a6_1440w.jpg" alt="img"></p>
<h3 id="主要问题"><a href="#主要问题" class="headerlink" title="主要问题"></a><strong>主要问题</strong></h3><ul>
<li>在估计值函数的时候一个任意小的变化可能导致对应动作被选择或者不被选择，这种不连续的变化是致使基于值函数的方法无法得到收敛保证的重要因素。</li>
<li>选择最大的Q值这样一个搜索过程在高纬度或者连续空间是非常困难的；</li>
<li>无法学习到<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=随机策略&amp;zhida_source=entity">随机策略</a>，有些情况下随机策略往往是最优策略。</li>
</ul>
<h3 id="示例代码-2"><a href="#示例代码-2" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear, Conv2d, ReLU</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span><span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 经验池</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DQBReplayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity</span>):</span><br><span class="line">        <span class="comment"># (S,A,R,S)</span></span><br><span class="line">        self.memory = pd.DataFrame(index=<span class="built_in">range</span>(capacity), columns=[<span class="string">&#x27;observation&#x27;</span>, <span class="string">&#x27;action&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>, <span class="string">&#x27;next_observation&#x27;</span>, <span class="string">&#x27;done&#x27;</span>])</span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store</span>(<span class="params">self,*args</span>):</span><br><span class="line">        self.memory.loc[self.i] = args</span><br><span class="line">        self.i = (self.i + <span class="number">1</span>)%self.capacity</span><br><span class="line">        self.count = <span class="built_in">min</span>(self.count+<span class="number">1</span>, self.capacity)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, size</span>):</span><br><span class="line">        indics = np.random.choice(self.count, size=size)</span><br><span class="line">        <span class="keyword">return</span> (np.stack(self.memory.loc[indics,field]) <span class="keyword">for</span> field <span class="keyword">in</span> self.memory.columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Q-Network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DQN_net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DQN_net, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">4</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">8</span>, stride=<span class="number">4</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>),</span><br><span class="line">            ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            Linear(<span class="number">3136</span>, <span class="number">512</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Linear(<span class="number">512</span>, <span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        output = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DQN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_shape, env</span>):</span><br><span class="line">        <span class="built_in">super</span>(DQN, self).__init__()</span><br><span class="line">        self.replayer_start_size = <span class="number">100000</span></span><br><span class="line">        self.upon_times = <span class="number">20</span></span><br><span class="line">        self.replayer = DQBReplayer(capacity=self.replayer_start_size)</span><br><span class="line">        self.action_n = env.action_space.n</span><br><span class="line">        self.image_stack = input_shape[<span class="number">2</span>]</span><br><span class="line">        self.gamma = <span class="number">0.99</span></span><br><span class="line">        self.image_shape = (input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>])</span><br><span class="line">        self.e_net = DQN_net()</span><br><span class="line">        self.t_net = DQN_net()</span><br><span class="line"></span><br><span class="line">        self.learn_step = <span class="number">0</span></span><br><span class="line">        self.max_learn_step = <span class="number">650000</span></span><br><span class="line">        self.epsilon = <span class="number">1.</span></span><br><span class="line">        self.start_learn = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_next_state</span>(<span class="params">self,state=<span class="literal">None</span>,observation=<span class="literal">None</span></span>):</span><br><span class="line">        img=Image.fromarray(observation,<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        img=img.resize(self.image_shape).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">        img=np.asarray(img.getdata(),dtype=np.uint8,).reshape(img.size[<span class="number">1</span>],img.size[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            next_state = np.array([img,]*self.image_stack)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_state = np.append(state[<span class="number">1</span>:],[img,],axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> next_state</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decide</span>(<span class="params">self,state,step</span>):</span><br><span class="line">        <span class="keyword">if</span> self.start_learn == <span class="literal">False</span>: <span class="comment">#前50000步随机选择</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">            <span class="keyword">return</span> action</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.epsilon -= <span class="number">0.0000053</span></span><br><span class="line">        <span class="keyword">if</span> step &lt; <span class="number">30</span>:</span><br><span class="line">            <span class="comment">#每局前三十步随机选择，中间30万，</span></span><br><span class="line">            <span class="comment">#以一定概率（1-epsilon）通过神经网络选择，</span></span><br><span class="line">            <span class="comment"># 最后30万次以0.99概率通过神经网络选择</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">elif</span> np.random.random() &lt; <span class="built_in">max</span>(self.epsilon, <span class="number">0.0005</span>):</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state = state/<span class="number">128</span> - <span class="number">1</span></span><br><span class="line">            y = torch.Tensor(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            x = self.e_net(y).detach()</span><br><span class="line">            <span class="keyword">if</span> self.learn_step%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;q value&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x))</span><br><span class="line">            action = torch.argmax(x).data.item()</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    sum_reward = <span class="number">0</span></span><br><span class="line">    store_count = <span class="number">0</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Breakout-v0&#x27;</span>)</span><br><span class="line">    net = DQN([<span class="number">84</span>,<span class="number">84</span>,<span class="number">4</span>], env).cuda()</span><br><span class="line">    </span><br><span class="line">    Load_Net = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> Load_Net==<span class="number">1</span>:</span><br><span class="line">        load_net_path = <span class="string">&#x27;./epsiode_2575_reward_10.0.pkl&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Load old net and the path is:&quot;</span>,load_net_path)</span><br><span class="line">        net.e_net = torch.load(load_net_path)</span><br><span class="line">        net.t_net = torch.load(load_net_path)</span><br><span class="line">    max_score = <span class="number">0</span></span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    mse = mse.cuda()</span><br><span class="line">    opt = torch.optim.RMSprop(net.e_net.parameters(), lr=<span class="number">0.0015</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">        lives = <span class="number">5</span></span><br><span class="line">        obs = env.reset()</span><br><span class="line">        state = net.get_next_state(<span class="literal">None</span>,obs)</span><br><span class="line">        epoch_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; times_game&quot;</span>.<span class="built_in">format</span>(i),end=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch_reward:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch_reward))</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500000</span>):</span><br><span class="line">            action = net.decide(state,step=step)</span><br><span class="line">            obs, reward, done, _ = env.step(action)</span><br><span class="line">            next_state = net.get_next_state(state, obs) </span><br><span class="line">            epoch_reward += reward</span><br><span class="line">            net.replayer.store(state, action, reward, next_state, done)</span><br><span class="line">            net.learn_step += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> net.learn_step &gt;= net.replayer_start_size // <span class="number">2</span> <span class="keyword">and</span> net.learn_step % <span class="number">4</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> net.start_learn == <span class="literal">False</span>:</span><br><span class="line">                    net.start_learn = <span class="literal">True</span></span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;Start Learn!&#x27;</span>)</span><br><span class="line">                sample_n = <span class="number">32</span></span><br><span class="line">                states, actions, rewards, next_states, dones = net.replayer.sample(sample_n)</span><br><span class="line">                states, next_states = states / <span class="number">128</span> -<span class="number">1</span>, next_states / <span class="number">128</span> -<span class="number">1</span></span><br><span class="line">                rewards = torch.Tensor(np.clip(rewards,-<span class="number">1</span>,<span class="number">1</span>)).unsqueeze(<span class="number">1</span>).cuda()</span><br><span class="line">                states, next_states = torch.Tensor(states).cuda(), torch.Tensor(next_states).cuda()</span><br><span class="line">                actions = torch.Tensor(actions).long().unsqueeze(<span class="number">1</span>).cuda()</span><br><span class="line">                dones = torch.Tensor(dones).unsqueeze(<span class="number">1</span>).cuda()</span><br><span class="line">                q = net.e_net(states).gather(<span class="number">1</span>, actions)</span><br><span class="line">                q_next = net.t_net(next_states).detach().<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>].reshape(sample_n, <span class="number">1</span>)</span><br><span class="line">                tq = rewards + net.gamma * (<span class="number">1</span>-done) * q_next</span><br><span class="line">                loss = mse(q, tq)</span><br><span class="line">                opt.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                opt.step()</span><br><span class="line">                <span class="keyword">if</span> net.learn_step % (net.upon_times * <span class="number">5</span>) == <span class="number">0</span>:</span><br><span class="line">                    net.t_net.load_state_dict(net.e_net.state_dict())</span><br><span class="line">                <span class="keyword">if</span> net.learn_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                    loss_record = loss.item()</span><br><span class="line">                    a_r = torch.mean(rewards, <span class="number">0</span>).item()</span><br><span class="line">                </span><br><span class="line">            state = next_state</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                save_net_path = <span class="string">&#x27;./&#x27;</span></span><br><span class="line">                sum_reward+=epoch_reward</span><br><span class="line">                <span class="keyword">if</span> epoch_reward &gt; max_score:</span><br><span class="line">                    name = <span class="string">&quot;epsiode_&quot;</span> + <span class="built_in">str</span>(net.learn_step) + <span class="string">&quot;_reward_&quot;</span> + <span class="built_in">str</span>(epoch_reward) + <span class="string">&quot;.pkl&quot;</span></span><br><span class="line">                    torch.save(net.e_net, save_net_path+name)</span><br><span class="line">                    max_score = epoch_reward</span><br><span class="line">                <span class="keyword">elif</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                    name =<span class="string">&quot;No.&quot;</span>+<span class="built_in">str</span>(i)+<span class="string">&quot;.pkl&quot;</span></span><br><span class="line">                    torch.save(net.e_net, save_net_path + name)</span><br><span class="line">                <span class="keyword">if</span> i%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">                    sum_reward=<span class="number">0</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">               </span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PictureArray2Video</span>(<span class="params">pic_list, path=<span class="string">&#x27;./test.mp4&#x27;</span></span>):</span><br><span class="line">    h,w,_ = pic_list[<span class="number">0</span>].shape[<span class="number">0</span>], pic_list[<span class="number">0</span>].shape[<span class="number">1</span>], pic_list[<span class="number">0</span>].shape[<span class="number">2</span>]</span><br><span class="line">    <span class="built_in">print</span>(h,w)</span><br><span class="line">    writer = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;v&#x27;</span>), <span class="number">10</span>, (w, h), <span class="literal">True</span>)</span><br><span class="line">    total_frame = <span class="built_in">len</span>(pic_list)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total_frame):</span><br><span class="line">        writer.write(pic_list[i])</span><br><span class="line">    writer.release()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    pics = []</span><br><span class="line">    sum_reward = <span class="number">0</span></span><br><span class="line">    store_count = <span class="number">0</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Breakout-v0&#x27;</span>)</span><br><span class="line">    net = DQN([<span class="number">84</span>,<span class="number">84</span>,<span class="number">4</span>], env).cuda()</span><br><span class="line">    </span><br><span class="line">    Load_Net = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> Load_Net==<span class="number">1</span>:</span><br><span class="line">        load_net_path = <span class="string">&#x27;./epsiode_10219_reward_9.0.pkl&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Load old net and the path is:&quot;</span>,load_net_path)</span><br><span class="line">        net.e_net = torch.load(load_net_path)</span><br><span class="line">        net.t_net = torch.load(load_net_path)</span><br><span class="line">    max_score = <span class="number">0</span></span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    mse = mse.cuda()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    obs = env.reset()</span><br><span class="line">    state = net.get_next_state(<span class="literal">None</span>,obs)</span><br><span class="line">    epoch_reward = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500000</span>):</span><br><span class="line">        action = net.decide(state,step=step)</span><br><span class="line">        obs, reward, done, _ = env.step(action)</span><br><span class="line">        pic = env.render(mode=<span class="string">&#x27;rgb_array&#x27;</span>)</span><br><span class="line">        pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)</span><br><span class="line">        next_state = net.get_next_state(state, obs) </span><br><span class="line">        pics.append(pic)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            PictureArray2Video(pics)</span><br><span class="line">            <span class="keyword">break</span>   </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a><strong>Policy Gradient</strong></h2><p>前面我们介绍的Q-Learning和DQN都是基于价值的强化学习算法，在给定一个状态下，计算采取每个动作的价值，我们选择有<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=最高Q值&amp;zhida_source=entity">最高Q值</a>（在所有状态下最大的期望奖励）的行动。如果我们省略中间的步骤，即直接根据当前的状态来选择动作，也就引出了强化学习中的另一种很重要的算法，即策略梯度(Policy Gradient， PG)</p>
<p>策略梯度不通过误差反向传播，它通过观测信息选出一个行为直接进行<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=2&amp;q=反向传播&amp;zhida_source=entity">反向传播</a>，当然出人意料的是他并没有误差，而是利用reward奖励直接对选择行为的可能性进行增强和减弱，好的行为会被增加下一次被选中的概率，不好的行为会被减弱下次被选中的概率。</p>
<p>举例如下图所示：输入当前的状态，输出action的概率分布，选择概率最大的一个action作为要执行的操作。</p>
<p><img src="https://pica.zhimg.com/v2-1a8a4baf9752a489ced36a2d31e17d34_1440w.jpg" alt="img"></p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a><strong>优缺点</strong></h3><p><strong>优点</strong></p>
<ul>
<li>连续的动作空间（或者高维空间）中更加高效；</li>
<li>可以实现随机化的策略；</li>
<li>某种情况下，价值函数可能比较难以计算，而策略函数较容易。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>通常收敛到<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=局部最优&amp;zhida_source=entity">局部最优</a>而非全局最优</li>
<li>评估一个策略通常低效（这个过程可能慢，但是具有更高的可变性，其中也会出现很多并不有效的尝试，而且<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=方差&amp;zhida_source=entity">方差</a>高）</li>
</ul>
<h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a><strong>REINFORCE</strong></h3><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=蒙特卡罗策略梯度&amp;zhida_source=entity">蒙特卡罗策略梯度</a>reinforce算法是策略梯度最简单的也是最经典的一个算法。</p>
<p><img src="https://pica.zhimg.com/v2-1f3d4dcda65437b6014c6dc2666e8876_1440w.jpg" alt="img"></p>
<h3 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h3><p><img src="https://pic1.zhimg.com/v2-3a520192dd65ed89afc29322ef23a94a_1440w.jpg" alt="img"></p>
<p>首先我们需要一个 policy model 来输出动作概率，输出动作概率后，我们 sample() 函数去得到一个具体的动作，然后跟环境交互过后，我们可以得到一整个回合的数据。拿到回合数据之后，我再去执行一下 learn() 函数，在 learn() 函数里面，我就可以拿这些数据去构造<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=损失函数&amp;zhida_source=entity">损失函数</a>，扔给这个优化器去优化，去更新我的 policy model。</p>
<h3 id="示例代码-3"><a href="#示例代码-3" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Categorical</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.002</span></span><br><span class="line">gamma = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PGPolicy</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size=<span class="number">4</span>, hidden_size=<span class="number">128</span>, output_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PGPolicy, self).__init__()</span><br><span class="line">        self.fc1 = Linear(input_size, hidden_size)</span><br><span class="line">        self.fc2 = Linear(hidden_size, output_size)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.6</span>)</span><br><span class="line">        </span><br><span class="line">        self.saved_log_probs = []<span class="comment"># 记录每一步的动作概率</span></span><br><span class="line">        self.rewards = []<span class="comment">#记录每一步的r</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        out = F.softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">state, policy</span>):</span><br><span class="line">    state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>) <span class="comment"># 在索引0对应位置增加一个维度</span></span><br><span class="line">    probs = policy(state) </span><br><span class="line">    m = Categorical(probs) <span class="comment">#创建以参数probs为标准的类别分布,之后的m.sampe就会按此概率选择动作</span></span><br><span class="line">    action = m.sample()</span><br><span class="line">    policy.saved_log_probs.append(m.log_prob(action))</span><br><span class="line">    <span class="keyword">return</span> action.item()<span class="comment">#返回的就是int</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">policy, optimizer</span>):</span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    returns = []</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> policy.rewards[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma*R</span><br><span class="line">        returns.insert(<span class="number">0</span>,R)<span class="comment">#从头部插入，即反着插入</span></span><br><span class="line">    returns = torch.tensor(returns)</span><br><span class="line">    <span class="comment"># 归一化（均值方差），eps是一个非常小的数，避免除数为0</span></span><br><span class="line">    eps = np.finfo(np.float64).eps.item()</span><br><span class="line">    returns = (returns - returns.mean()) / (returns.std() + eps)  </span><br><span class="line">    <span class="keyword">for</span> log_prob, R <span class="keyword">in</span> <span class="built_in">zip</span>(policy.saved_log_probs, returns):</span><br><span class="line">        policy_loss.append(-log_prob*R)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()</span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> policy.rewards[:]  <span class="comment"># 清空数据</span></span><br><span class="line">    <span class="keyword">del</span> policy.saved_log_probs[:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">episode_num</span>):</span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">    env.seed(<span class="number">1</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">    policy = PGPolicy()</span><br><span class="line">    <span class="comment"># policy.load_state_dict(torch.load(&#x27;save_model.pt&#x27;))  # 模型导入</span></span><br><span class="line">    optimizer = optim.Adam(policy.parameters(), lr)</span><br><span class="line">    average_r = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, episode_num+<span class="number">1</span>): <span class="comment">#采这么多轨迹</span></span><br><span class="line">        obs = env.reset()</span><br><span class="line">        ep_r = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):</span><br><span class="line">            action = choose_action(obs, policy)</span><br><span class="line">            obs, reward, done, _ = env.step(action)</span><br><span class="line">            policy.rewards.append(reward)</span><br><span class="line">            ep_r += reward</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        average_r = <span class="number">0.05</span> * ep_r + (<span class="number">1</span>-<span class="number">0.05</span>) * average_r</span><br><span class="line">        learn(policy, optimizer)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(i, ep_r, average_r))</span><br><span class="line"></span><br><span class="line">    torch.save(policy.state_dict(), <span class="string">&#x27;PGPolicy.pt&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">    env.seed(<span class="number">1</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">    policy = PGPolicy()</span><br><span class="line">    policy.load_state_dict(torch.load(<span class="string">&#x27;PGPolicy.pt&#x27;</span>))  <span class="comment"># 模型导入</span></span><br><span class="line">    average_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        obs = env.reset()</span><br><span class="line">        ep_r = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):</span><br><span class="line">            action = choose_action(obs, policy)</span><br><span class="line">            obs, reward, done, _ = env.step(action)</span><br><span class="line">            policy.rewards.append(reward)</span><br><span class="line">            env.render()</span><br><span class="line">            time.sleep(<span class="number">0.1</span>)</span><br><span class="line">            ep_r += reward</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">train(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  test()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a><strong>Actor Critic</strong></h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=演员-评论家算法&amp;zhida_source=entity">演员-评论家算法</a>(Actor-Critic)是基于策略(Policy Based)和基于价值(Value Based)相结合的方法</p>
<p><img src="https://picx.zhimg.com/v2-30c86bf5852c7278739988deb843fa2b_1440w.jpg" alt="img"></p>
<ul>
<li>演员(Actor)是指<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=2&amp;q=策略函数&amp;zhida_source=entity">策略函数</a>πθ(a|s)，即学习一个策略来得到尽量高的回报。</li>
<li>评论家(Critic)是指值函数 Vπ(s)，对当前策略的值函数进行估计，即评估演员的好坏。</li>
<li>借助于价值函数，演员-评论家算法可以进行单步更新参数，不需要等到回合结束才进行更新。</li>
</ul>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a><strong>网络结构</strong></h3><p>整体结构：</p>
<p><img src="https://pic2.zhimg.com/v2-6e27d52ea15a03b569ee21d504bf0cc1_1440w.jpg" alt="img"></p>
<p>Actor和Critic的网络结构：</p>
<p><img src="https://pic2.zhimg.com/v2-c3fef2d159d761a2a97c29c53acad071_1440w.jpg" alt="img"></p>
<h3 id="算法流程-3"><a href="#算法流程-3" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h3><p><img src="https://pic1.zhimg.com/v2-33e2dbe0670759efcb823bf02dda8c06_1440w.jpg" alt="img"></p>
<h3 id="问题和改进"><a href="#问题和改进" class="headerlink" title="问题和改进"></a><strong>问题和改进</strong></h3><p>Actor Critic 取决于 Critic 的价值判断， 但是 Critic 难收敛， 再加上 Actor 的更新， 就更难收敛，为了解决该问题又提出了 A3C 算法和 DDPG 算法。</p>
<p><strong>改进算法1：A3C</strong> 异步的优势<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=行动者评论家算法&amp;zhida_source=entity">行动者评论家算法</a>（Asynchronous Advantage Actor-Critic，A3C），相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p><img src="https://pica.zhimg.com/v2-56823ffd3980eb9a984676176b83384a_1440w.jpg" alt="img"></p>
<p><strong>改进算法2：DDPG</strong> 深度确定性策略梯度(Deep Deterministic Policy Gradient，DDPG)，从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。</p>
<ul>
<li>Deep 是因为用了神经网络；</li>
<li>Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；</li>
<li>Policy Gradient 代表的是它用到的是<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhida.zhihu.com/search?content_id=191787601&amp;content_type=Article&amp;match_order=1&amp;q=策略网络&amp;zhida_source=entity">策略网络</a>。REINFORCE 算法每隔一个 episode 就更新一次，但 DDPG 网络是每个 step 都会更新一次 policy 网络，也就是说它是一个单步更新的 policy 网络。</li>
</ul>
<p><img src="https://pic2.zhimg.com/v2-0bce8174e6dd2b4851ad4c90a86f9967_1440w.jpg" alt="img"></p>
<h3 id="示例代码-4"><a href="#示例代码-4" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################  hyper parameters  ####################</span></span><br><span class="line">EPISODES = <span class="number">200</span></span><br><span class="line">EP_STEPS = <span class="number">200</span></span><br><span class="line">LR_ACTOR = <span class="number">0.001</span></span><br><span class="line">LR_CRITIC = <span class="number">0.002</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">TAU = <span class="number">0.01</span></span><br><span class="line">MEMORY_CAPACITY = <span class="number">10000</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">RENDER = <span class="literal">False</span></span><br><span class="line">ENV_NAME = <span class="string">&#x27;Pendulum-v1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################## DDPG Framework ######################</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ActorNet</span>(nn.Module): <span class="comment"># define the network structure for actor and critic</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, s_dim, a_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(ActorNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(s_dim, <span class="number">30</span>)</span><br><span class="line">        self.fc1.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>) <span class="comment"># initialization of FC1</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">30</span>, a_dim)</span><br><span class="line">        self.out.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>) <span class="comment"># initilizaiton of OUT</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.out(x)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        actions = x * <span class="number">2</span> <span class="comment"># for the game &quot;Pendulum-v0&quot;, action range is [-2, 2]</span></span><br><span class="line">        <span class="keyword">return</span> actions</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CriticNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, s_dim, a_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(CriticNet, self).__init__()</span><br><span class="line">        self.fcs = nn.Linear(s_dim, <span class="number">30</span>)</span><br><span class="line">        self.fcs.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.fca = nn.Linear(a_dim, <span class="number">30</span>)</span><br><span class="line">        self.fca.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">30</span>, <span class="number">1</span>)</span><br><span class="line">        self.out.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, s, a</span>):</span><br><span class="line">        x = self.fcs(s)</span><br><span class="line">        y = self.fca(a)</span><br><span class="line">        actions_value = self.out(F.relu(x+y))</span><br><span class="line">        <span class="keyword">return</span> actions_value</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DDPG</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a_dim, s_dim, a_bound</span>):</span><br><span class="line">        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound</span><br><span class="line">        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * <span class="number">2</span> + a_dim + <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">        self.pointer = <span class="number">0</span> <span class="comment"># serves as updating the memory data </span></span><br><span class="line">        <span class="comment"># Create the 4 network objects</span></span><br><span class="line">        self.actor_eval = ActorNet(s_dim, a_dim)</span><br><span class="line">        self.actor_target = ActorNet(s_dim, a_dim)</span><br><span class="line">        self.critic_eval = CriticNet(s_dim, a_dim)</span><br><span class="line">        self.critic_target = CriticNet(s_dim, a_dim)</span><br><span class="line">        <span class="comment"># create 2 optimizers for actor and critic</span></span><br><span class="line">        self.actor_optimizer = torch.optim.Adam(self.actor_eval.parameters(), lr=LR_ACTOR)</span><br><span class="line">        self.critic_optimizer = torch.optim.Adam(self.critic_eval.parameters(), lr=LR_CRITIC)</span><br><span class="line">        <span class="comment"># Define the loss function for critic network update</span></span><br><span class="line">        self.loss_func = nn.MSELoss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store_transition</span>(<span class="params">self, s, a, r, s_</span>): <span class="comment"># how to store the episodic data to buffer</span></span><br><span class="line">        transition = np.hstack((s, a, [r], s_))</span><br><span class="line">        index = self.pointer % MEMORY_CAPACITY <span class="comment"># replace the old data with new data </span></span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.pointer += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="comment"># print(s)</span></span><br><span class="line">        s = torch.unsqueeze(torch.FloatTensor(s), <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.actor_eval(s)[<span class="number">0</span>].detach()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># softly update the target networks</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> self.actor_target.state_dict().keys():</span><br><span class="line">            <span class="built_in">eval</span>(<span class="string">&#x27;self.actor_target.&#x27;</span> + x + <span class="string">&#x27;.data.mul_((1-TAU))&#x27;</span>)</span><br><span class="line">            <span class="built_in">eval</span>(<span class="string">&#x27;self.actor_target.&#x27;</span> + x + <span class="string">&#x27;.data.add_(TAU*self.actor_eval.&#x27;</span> + x + <span class="string">&#x27;.data)&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> self.critic_target.state_dict().keys():</span><br><span class="line">            <span class="built_in">eval</span>(<span class="string">&#x27;self.critic_target.&#x27;</span> + x + <span class="string">&#x27;.data.mul_((1-TAU))&#x27;</span>)</span><br><span class="line">            <span class="built_in">eval</span>(<span class="string">&#x27;self.critic_target.&#x27;</span> + x + <span class="string">&#x27;.data.add_(TAU*self.critic_eval.&#x27;</span> + x + <span class="string">&#x27;.data)&#x27;</span>)           </span><br><span class="line">        <span class="comment"># sample from buffer a mini-batch data</span></span><br><span class="line">        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)</span><br><span class="line">        batch_trans = self.memory[indices, :]</span><br><span class="line">        <span class="comment"># extract data from mini-batch of transitions including s, a, r, s_</span></span><br><span class="line">        batch_s = torch.FloatTensor(batch_trans[:, :self.s_dim])</span><br><span class="line">        batch_a = torch.FloatTensor(batch_trans[:, self.s_dim:self.s_dim + self.a_dim])</span><br><span class="line">        batch_r = torch.FloatTensor(batch_trans[:, -self.s_dim - <span class="number">1</span>: -self.s_dim])</span><br><span class="line">        batch_s_ = torch.FloatTensor(batch_trans[:, -self.s_dim:])</span><br><span class="line">        <span class="comment"># make action and evaluate its action values</span></span><br><span class="line">        a = self.actor_eval(batch_s)</span><br><span class="line">        q = self.critic_eval(batch_s, a)</span><br><span class="line">        actor_loss = -torch.mean(q)</span><br><span class="line">        <span class="comment"># optimize the loss of actor network</span></span><br><span class="line">        self.actor_optimizer.zero_grad()</span><br><span class="line">        actor_loss.backward()</span><br><span class="line">        self.actor_optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute the target Q value using the information of next state</span></span><br><span class="line">        a_target = self.actor_target(batch_s_)</span><br><span class="line">        q_tmp = self.critic_target(batch_s_, a_target)</span><br><span class="line">        q_target = batch_r + GAMMA * q_tmp</span><br><span class="line">        <span class="comment"># compute the current q value and the loss</span></span><br><span class="line">        q_eval = self.critic_eval(batch_s, batch_a)</span><br><span class="line">        td_error = self.loss_func(q_target, q_eval)</span><br><span class="line">        <span class="comment"># optimize the loss of critic network</span></span><br><span class="line">        self.critic_optimizer.zero_grad()</span><br><span class="line">        td_error.backward()</span><br><span class="line">        self.critic_optimizer.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment">############################### Training ######################################</span></span><br><span class="line"><span class="comment"># Define the env in gym</span></span><br><span class="line">env = gym.make(ENV_NAME)</span><br><span class="line">env = env.unwrapped</span><br><span class="line">env.seed(<span class="number">1</span>)</span><br><span class="line">s_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">a_dim = env.action_space.shape[<span class="number">0</span>]</span><br><span class="line">a_bound = env.action_space.high</span><br><span class="line">a_low_bound = env.action_space.low</span><br><span class="line"></span><br><span class="line">ddpg = DDPG(a_dim, s_dim, a_bound)</span><br><span class="line">var = <span class="number">3</span> <span class="comment"># the controller of exploration which will decay during training process</span></span><br><span class="line">t1 = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(EPISODES):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    ep_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(EP_STEPS):</span><br><span class="line">        <span class="keyword">if</span> RENDER: env.render()</span><br><span class="line">        <span class="comment"># add explorative noise to action</span></span><br><span class="line">        a = ddpg.choose_action(s)</span><br><span class="line">        a = np.clip(np.random.normal(a, var), a_low_bound, a_bound)</span><br><span class="line">        s_, r, done, info, _ = env.step(a)</span><br><span class="line">        ddpg.store_transition(s, a, r / <span class="number">10</span>, s_) <span class="comment"># store the transition to memory</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> ddpg.pointer &gt; MEMORY_CAPACITY:</span><br><span class="line">            var *= <span class="number">0.9995</span> <span class="comment"># decay the exploration controller factor</span></span><br><span class="line">            ddpg.learn()</span><br><span class="line">            </span><br><span class="line">        s = s_</span><br><span class="line">        ep_r += r</span><br><span class="line">        <span class="keyword">if</span> j == EP_STEPS - <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Episode: &#x27;</span>, i, <span class="string">&#x27; Reward: %i&#x27;</span> % (ep_r), <span class="string">&#x27;Explore: %.2f&#x27;</span> % var)</span><br><span class="line">            <span class="keyword">if</span> ep_r &gt; -<span class="number">300</span> : RENDER = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Running time: &#x27;</span>, time.time() - t1)</span><br><span class="line">    </span><br><span class="line">       </span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    learn()    </span><br><span class="line">    env.close()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1].<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/466455380#:~:text=强化学习（Reinforcement learning，RL）讨论的问题是一个 智能体 (agent">强化学习入门：基本思想和经典算法 - 知乎</a> 怎么在一个复杂不确定的 环境 (environment) 里面去极大化它能获得的奖励。,动作 (action) 的 反应 (reward)， 来指导更好的动作，从而获得最大的 收益 (return)，这被称为在交互中学习，这样的学习方法就被称作强化学习。)</p>
<p>[2].<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_45696231/article/details/126723207">基础的强化学习(RL)算法及代码详细demo_强化学习代码-CSDN博客</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://fezhu.top">Doraemon</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://fezhu.top/posts/3ff4588e.html">http://fezhu.top/posts/3ff4588e.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://fezhu.top" target="_blank">Doraemon's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/uncleacc/Img2/130.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/340c83dd.html" title="jvm内存模型"><img class="cover" src="https://cdn.jsdelivr.net/gh/uncleacc/Img2/129.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">jvm内存模型</div></div></a></div><div class="next-post pull-right"><a href="/posts/244d313a.html" title="Cookie技术介绍"><img class="cover" src="https://cdn.jsdelivr.net/gh/uncleacc/Img2/131.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Cookie技术介绍</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E6%83%B3"><span class="toc-number">2.</span> <span class="toc-text">强化学习思想</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text">相关概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E6%83%B3"><span class="toc-number">2.2.</span> <span class="toc-text">思想</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">经典算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Sarsa-%E6%82%AC%E5%B4%96%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.</span> <span class="toc-text">Sarsa (悬崖问题)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">3.1.1.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%80%BC%E8%A1%A8"><span class="toc-number">3.2.1.</span> <span class="toc-text">Q值表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%87%BD%E6%95%B0"><span class="toc-number">3.2.2.</span> <span class="toc-text">Q函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-1"><span class="toc-number">3.2.4.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Q-Network"><span class="toc-number">3.3.</span> <span class="toc-text">Deep Q Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">3.3.1.</span> <span class="toc-text">经验回放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9AQ%E7%9B%AE%E6%A0%87"><span class="toc-number">3.3.2.</span> <span class="toc-text">固定Q目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-1"><span class="toc-number">3.3.3.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98"><span class="toc-number">3.3.4.</span> <span class="toc-text">主要问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-2"><span class="toc-number">3.3.5.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">3.4.</span> <span class="toc-text">Policy Gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">3.4.1.</span> <span class="toc-text">优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#REINFORCE"><span class="toc-number">3.4.2.</span> <span class="toc-text">REINFORCE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-2"><span class="toc-number">3.4.3.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-3"><span class="toc-number">3.4.4.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic"><span class="toc-number">3.5.</span> <span class="toc-text">Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">3.5.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-3"><span class="toc-number">3.5.2.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%92%8C%E6%94%B9%E8%BF%9B"><span class="toc-number">3.5.3.</span> <span class="toc-text">问题和改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-4"><span class="toc-number">3.5.4.</span> <span class="toc-text">示例代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">4.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By Doraemon</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://fezhu.top/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'fbMkGCqFR4CUHufZzPUvIprm-MdYXbMMI',
      appKey: '3J39KYN5r1aiDyQofBtAPNNj',
      avatar: '',
      serverURLs: 'https://fbmkgcqf.api.lncldglobal.com',
      emojiMaps: "",
      master: '71a69a31286a700ee0738a6e0942e102',
      tagMeta: ["博主","小伙伴","访客"],
      friends:  [], 
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>