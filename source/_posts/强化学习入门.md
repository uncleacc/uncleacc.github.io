---
title: 强化学习入门
mathjax: true
tags: 强化学习
cover: 'https://cdn.jsdelivr.net/gh/uncleacc/Img2/139.webp'
categories: 算法
abbrlink: 514049a
keywords:
  - Qlearning
  - DQN
  - DDPG
  - 强化学习
date: 2025-11-08 22:47:44
updated:
description:
comments:
highlight_shrink:
---

### 理论基础

- **MDP（马尔可夫模型）**：当前状态只和前面一个状态有关，目的是为了简化学习过程，认为当前状态就包含了过去的信息。与之相对的就是非马尔可夫过程，认为当前状态和前面的所有状态有关。
- **POMDP（部分马尔可夫可观测决策模型）**：POMDP 是一种**智能体无法直接观测到真实状态**的决策模型。虽然系统的内部状态 **仍然满足马尔可夫性**（这点很关键），但智能体获得的只是状态的**部分信息（观测值）**，因此对智能体来说，环境**表现为非马尔可夫的**。

| 模型      | 场景比喻                                                     |
| --------- | ------------------------------------------------------------ |
| **MDP**   | 你在明亮的房间里走迷宫，看得清周围墙壁的位置，决策只需看当前。 |
| **POMDP** | 你在黑暗中走迷宫，只能凭手电照到的一小块区域和脚步声猜测位置。环境本身仍遵守马尔可夫性，但你“看不全”。 |

- **状态价值函数**：当智能体在状态 s 下，按照策略 π 行动后，**能获得的期望总回报**。公式：$V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$。
- **动作价值函数**：当智能体在状态 s 下采取动作 a，然后**再按照策略 π 行动**，能获得的期望总回报。$Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$。状态价值函数可以看作动作价值函数的“平均值”：$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a)$。
- **贝尔曼方程（核心公式）**：一种特殊的状态价值函数。用估计值去优化估计值，可以收敛的原因在于终态的价值函数是已知的就是0，那么终态前一个状态就可以用真实价值进行反推优化，同理上上个状态也可以，在迭代足够次数后所有状态都会趋向真实值。$Q^*(s, a) = E\left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right]$，意义：即时奖励+下一个状态的最大价值乘以折扣因子（因为时间也是成本，未来的奖励相较于即时奖励应该打折扣），最后取个期望，即估计值。
- **时序差分（Temporal Difference, TD）学习 VS 蒙特卡洛（Monte Carlo，MC）方法**：**MC 用“真实未来”更新；TD 用“预测未来”更新**。

| 方法               | 核心思想                            | 样本使用               | 更新时机         | 优缺点                                             |
| ------------------ | ----------------------------------- | ---------------------- | ---------------- | -------------------------------------------------- |
| **蒙特卡洛（MC）** | 用完整回合的真实累计回报来更新价值  | 一条完整轨迹一次性更新 | 需要等待回合结束 | ✅ 无偏估计，准确；❌ 方差大，不能在线更新           |
| **时序差分（TD）** | 用一步预测 + 下一步价值近似未来回报 | 当前步 + 下一步        | 每一步都可更新   | ✅ 在线更新，样本效率高；❌ 有偏估计，依赖预测值质量 |

- **策略（Policy，记作 π）**：智能体在每个状态下选择动作的规则或概率分布。
  - **确定性策略**（Deterministic Policy）：a=π(s)，表示状态 s 下智能体总是选择固定动作 a。
  - **随机策略**（Stochastic Policy）：a∼π(a∣s)，表示状态 s 下智能体以一定概率选择动作 a。

### 经典算法

1. 基于价值函数（Value-Based）

- Q-Learning：无模型时序差分算法，通过更新 Q 表学习动作价值，适用于离散动作空间。
- SARSA：同属时序差分算法，采用 “在线学习”（执行动作后更新），更注重策略安全性。
- DQN（Deep Q-Network）：结合深度学习与 Q-Learning，用神经网络替代 Q 表，适配高维状态空间。
- Double DQN / Dueling DQN：DQN 的改进版，分别解决过估计问题和分离价值与优势函数，提升稳定性。

2. 基于策略梯度（Policy-Based）

- REINFORCE：蒙特卡洛策略梯度算法，直接优化策略函数，适用于离散 / 连续动作空间。
- A2C（Advantage Actor-Critic）：结合策略网络（Actor）和价值网络（Critic），通过优势函数减少方差，提升训练效率。
- A3C（Asynchronous A2C）：A2C 的异步版本，多线程并行训练，加速探索与收敛。

3. 演员 - 评论员（Actor-Critic）与连续动作适配

- DDPG（Deep Deterministic Policy Gradient）：专为连续动作空间设计，用确定性策略替代随机策略，搭配目标网络和经验回放提升稳定性。
- PPO（Proximal Policy Optimization）：通过 “信任区域” 限制策略更新幅度，兼顾性能与稳定性，适用于离散 / 连续动作，落地场景广泛。
- TD3（Twin Delayed DDPG）：DDPG 的改进版，双 Critic 网络减少过估计，延迟策略更新，优化连续动作学习效果。

基于价值函数只能处理离散动作空间，因为需要枚举动作计算 Q 值；

基于策略梯度可以处理连续动作空间，有两种方式，可以输出确定性动作，例如方向盘直接转动 45.6 度，也可以输出动作概率分布曲线，例如力度 40 牛的概率最高（曲线峰值），38-42 牛的概率稍低。这里和 Q 值的动作概率分布区别就是这里的动作是连续的，可以理解为连续的曲线，而 Q 值的动作概率分布是离散的点。



### 训练流程

强化学习训练的通用流程可概括为 “环境建模→智能体初始化→交互采样→策略更新→迭代收敛” 五步，核心是通过 “试错 - 反馈 - 优化” 循环提升策略性能，具体如下：

1. 环境与问题建模

- 定义核心要素：明确状态空间（环境可观测信息，如 MEC 基站缓存状态）、动作空间（智能体可执行操作，如缓存优先级调整）、奖励函数（目标量化，如延迟降低量）。
- 确定约束条件：设定物理或业务限制（如基站缓存容量、动作执行边界）。
- 选择环境类型：区分真实环境（如实际网络）或仿真环境（如基于数据集的模拟场景）。

2. 智能体架构设计

- 选择学习框架：基于问题类型确定价值型（如 DQN）、策略型（如 REINFORCE）或 Actor-Critic 型（如 DDPG）架构。
- 初始化网络 / 参数：搭建神经网络（如 Q 网络、策略网络），初始化权重、经验回放池（用于无模型算法）等。
- 设定探索策略：确定动作选择规则（如 ε-greedy、随机噪声），平衡探索新动作与利用已有经验。

3. 交互采样与数据收集

- 智能体观测当前状态（s），根据当前策略选择动作（a）。
- 执行动作后，获取环境反馈的即时奖励（r）和下一状态（s'）。
- 将经验元组（s, a, r, s'）存入经验回放池，为后续更新积累数据。
- 重复上述步骤，持续收集交互数据，直至满足单轮采样数量或触发终止条件。

4. 策略与价值网络更新

- 从经验回放池采样批量数据（无模型算法）或利用环境模型生成数据（有模型算法）。
- 计算目标值：基于奖励函数和折扣因子，计算动作价值目标（如 Q 目标值）或策略目标（如优势函数）。
- 反向传播优化：通过梯度下降最小化预测值与目标值的误差，更新价值网络（如 Q 网络）或策略网络（如 Actor 网络）。
- 同步目标网络（如 DDPG、DQN）：定期软更新或硬更新目标网络参数，保证训练稳定性。

5. 迭代训练与收敛验证

- 重复 “交互采样→网络更新” 流程，迭代多轮训练（episode）。
- 监控关键指标：跟踪奖励值、目标指标（如缓存命中率、延迟）的变化趋势。
- 判定收敛：当指标趋于稳定（如连续多轮奖励波动小于阈值），停止训练，保存最优模型参数。

例如 DQN 的训练过程如下：

<img src="https://dora-blog.oss-cn-beijing.aliyuncs.com/image-20251108222814648.png" alt="image-20251108222814648" style="zoom:50%;" />

### 常见问题

1. Qtarget 的作用是什么？

   Qtarget（目标 Q 值）用于**稳定训练目标**，防止网络在更新时“自己追自己”导致发散。如果训练目标也用当前网络参数 θ，那么每次更新都会影响目标值本身，
   就会形成**正反馈震荡**，使训练过程不稳定。所以，DQN 使用一个延迟更新的 **目标网络（Target Network）**：

   - 主网络（Online Network）：Q(s,a; θ) 负责学习；
   - 目标网络（Target Network）：Q_target(s,a; θ⁻) 提供稳定目标。

2. 为什么需要经验池？

   经验池的作用是**打破数据相关性**，**提高样本利用率**。这些数据是**时间相关的**（状态连续），而神经网络的训练假设样本是**独立同分布（i.i.d.）**。
   如果直接用序列样本训练，网络容易陷入震荡或遗忘。

   经验池的两个关键作用：

   1. **打乱顺序（随机取样）** → 让样本近似独立；
   2. **重复利用历史经验** → 提高样本效率。

3. 目标网络的软更新和回合更新区别是什么？

   两者都是为了更新目标网络参数 θ⁻，区别在于**更新频率和方式**。

   - 回合更新：每隔固定步数（如 1000 步）直接拷贝一次；更新突变，稳定但不连续。
   - 软更新：每一步都缓慢靠近主网络；连续、平滑，震荡更小；常见于 **DDPG、SAC 等连续控制算法**。

4. DQN 和 DDPG 分别用于处理什么问题？

   - DQN 适用于动作有限的情况，比如：“向上 / 向下 / 向左 / 向右”；

   - DDPG 适用于动作连续的情况，比如机器人控制“转角 37.5°”或“推力 0.68”。

5. 经验回放为什么要随机取样？

   - 环境交互数据是时间相关的；

   - 连续采样的样本高度相关，容易导致梯度方向相似 → 学习效率低；

   - 随机采样可打乱时间依赖，让网络每次学习到更全面的信息。



### 参考文献

[DQN算法原理及代码实现](https://mp.weixin.qq.com/s/wiiXFEw_gv5iQj36t80pgg)